{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b509e99c",
      "metadata": {
        "id": "b509e99c"
      },
      "source": [
        "## Scipy and Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d84e61",
      "metadata": {
        "id": "81d84e61"
      },
      "source": [
        "In the last notebook we took a tour of the `numpy` and `scipy` stack as it relates to math and statistics. This notebook is going to focus on another subset of `scipy` functionality that is absolutely critical to data analytics. Nearly every statistical algorithm aside from Ordinary Least Squares (OLS) regression cannot be solved algebraically. Because they cannot be solved algebraically, they must be solved **numerically**.\n",
        "\n",
        "### Numeric Optimization\n",
        "\n",
        "Let's start with an example of optimization. Let's say that you know the demand function for tickets to watch a local sports franchise. You can write the inverse demand function as\n",
        "\n",
        "$$ P = 300 - \\frac{1}{2}Q $$\n",
        "\n",
        "and the total cost function as\n",
        "\n",
        "$$ TC = 4000 + 45Q $$\n",
        "\n",
        "In order to choose the right number of tickets to sell, you need to calculate the quantity of tickets that will maximize profits. We can calculate total revenue as $ TR = P \\times Q $, and we can calculate profit as $ \\Pi = TR - TC $. This means that our profit function is\n",
        "\n",
        "$$ \\Pi = 300Q - \\frac{1}{2}Q^2 - 4000 - 45Q $$\n",
        "\n",
        "In order to find the $Q$ associated with the highest achievable level of profit, we can use calculus to find the point at which the rate of change in the profit function is zero ($\\frac{\\partial\\Pi}{\\partial Q}=0$).\n",
        "\n",
        "$$ \\frac{\\partial\\Pi}{\\partial Q} = 300 - Q - 45 = 0 \\implies Q = 255$$\n",
        "\n",
        "So we can **algebraically** solve this particular problem. This isn't always the case. Using `scipy`, we can solve this same problem, as well as many algebraically intractable problems that might be more interesting to us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6d805e4",
      "metadata": {
        "id": "c6d805e4"
      },
      "source": [
        "<img src=\"https://github.com/dustywhite7/Econ8320/raw/master/SlidesCode/paraboloid.png\" width=\"200\" height=\"200\" />\n",
        "\n",
        "\n",
        "In any optimization problem, we need to find a way to get ourselves to the minimum, and to know when we get there. When we look at the above image, we are able to visually trace the functional shape (looks like a rainbow ice cream cone to me...) and locate the bottom of the function. What we want to do is utilize an algorithm to \"trace\" our way from an arbitrary starting point within a function to the optimal point in that function.\n",
        "\n",
        "In three or fewer dimensions, this is easy. Regressions and statistical models often live in worlds with 100's or 1000's (even millions sometimes) of dimensions. We can't visualize our way to the bottom of those functions!\n",
        "\n",
        "The class of algorithm that is used to solve these problems is called **gradient descent**.\n",
        "\n",
        "<img src=\"https://github.com/dustywhite7/Econ8320/raw/master/SlidesCode/gradDesc.png\" width=\"400\" />\n",
        "\n",
        "**Gradient descent** is an algorithm that explores the shape of the function, and determines which direction is most likely to lead to the optimal point. Let's focus on minimization. We want to find our way to the *bottom* of a function, and we can use gradient descent to try to get there. Given any starting point, our goal is to check the direction in which we can move downward most quickly, and start moving in that direction. At some distance from our starting point, we will stop and re-evaluate the direction in which we would like to travel. Are we still heading downhill in the steepest direction? If we aren't, then we need to update our behavior.\n",
        "\n",
        "Our gradient descent algorithm will keep \"looking around\" and moving down until it reaches a point at which it can no longer move \"down\" in any meaningful way. That is the stopping point, and is treated as the optimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af96154",
      "metadata": {
        "id": "0af96154"
      },
      "source": [
        "With an intuitive understanding of how optimization will happen computationally, it's time to learn a bit more about the math and the code that will help us to achieve computational optimization.\n",
        "\n",
        "Consider a function, $f$, with two variables $x$ and $y$. Because there are two input variables in the function, it has two partial derivatives:\n",
        "\n",
        "$$ \\frac{\\partial f}{\\partial x} \\text{ and } \\frac{\\partial f}{\\partial y} $$\n",
        "\n",
        "Each partial derivative tells us how $f$ changes as we move in a particular dimension **all else equal**. The **gradient**, then, is the vector of all partial derivatives of a given function at any point along the function:\n",
        "\n",
        "$$ \\nabla f = \\left[ \\begin{matrix} \\frac{\\partial f}{\\partial x} \\\\ \\\\ \\frac{\\partial f}{\\partial y} \\end{matrix} \\right]  $$\n",
        "\n",
        "We can use the gradient to determine the linear approximation of a function at any given point. Think about the gradient as the mathematical representation of the slope and direction of a hill you are hiking on. If we know the gradient, we know which way is down. As we continue to calculate gradients while walking, we can continue to ensure that we will walk downhill until we reach the bottom of the hill.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b3fefac",
      "metadata": {
        "id": "6b3fefac"
      },
      "source": [
        "The steps of our gradient descent function will be the following:\n",
        "\n",
        "- Evaluate the gradient of the function\n",
        "- Find the direction of steepest descent\n",
        "- Determine how far to move in that direction\n",
        "- Move to new point\n",
        "- Reevaluate the gradient\n",
        "- Stop moving when gradient is within a margin of error from 0\n",
        "\n",
        "Let's try to implement gradient descent by solving our old profit maximization problem computationally. The very first thing that we need to do is write a Python function that will represent our mathematical function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c641a5e",
      "metadata": {
        "id": "3c641a5e"
      },
      "outputs": [],
      "source": [
        "def profit(q):\n",
        "    p = 300-0.5*q\n",
        "    tr = p*q\n",
        "    tc = 4000 + 45*q\n",
        "    return tr - tc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed1c0514",
      "metadata": {
        "id": "ed1c0514"
      },
      "source": [
        "This function will allow us to calculate profit at any output level based on our assumed total costs and demand curve. With this function, we can quickly calculate the gradient (in this case, just a simple derivative because our function is univariate) by calculating profit at two nearby points, and dividing by the horizontal distance between those points:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef8c0ac",
      "metadata": {
        "id": "4ef8c0ac"
      },
      "outputs": [],
      "source": [
        "# Gradient at q=200\n",
        "\n",
        "(profit(201) - profit(199))/2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e219ac5",
      "metadata": {
        "id": "5e219ac5"
      },
      "source": [
        "    55.0\n",
        "\n",
        "\n",
        "\n",
        "Thus, a one unit increase in output at $Q=200$ results in a $55 increase in profits. This is cool, but it isn't enough for us to find the point of maximum profit (the optimal point). For that, we will need to calculate LOTS of gradients in order to move along the function until we cannot increase profits any further.\n",
        "\n",
        "Fortunately for us, `scipy` comes with optimization tools that will do all of the heavy lifting of the \"search\" for the optimal point. All that we need to do is frame our question algorithmically, and let `scipy` do the rest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edaa6713",
      "metadata": {
        "id": "edaa6713"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b4f0c01",
      "metadata": {
        "id": "6b4f0c01"
      },
      "source": [
        "We start by importing the `minimize` function from `scipy.optimize`. Hang on! Weren't we working on a MAXIMIZATION problem?? What are we doing here?\n",
        "\n",
        "Maximization and minimization are the **same thing**. To maximize a function, you can multiply that function by `-1` and then calculate the minimum of the new \"upside-down\" function. It is functionally equivalent. So, in computational optimization, we always minimize.\n",
        "\n",
        "### Prepping for optimization\n",
        "\n",
        "As we prepare to optimize, there are two common problems with our function that we may need to resolve:\n",
        "\n",
        "1) When using `minimize` we can only pass an array of inputs, so we have to be careful to write our function accordingly\n",
        "2) Our problem is concave, and so has a maximum\n",
        "\t- We need to restate it as a minimization problem\n",
        "\n",
        "Problem 1 does not apply, since our function in univariate. In order to make our problem a minimization problem, we can flip our profit maximization function. We will simply return -1 * profit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c950291a",
      "metadata": {
        "id": "c950291a"
      },
      "outputs": [],
      "source": [
        "def profit(q):\n",
        "    p = 300-0.5*q\n",
        "    tr = p*q\n",
        "    tc = 4000 + 45*q\n",
        "    pi =  tr - tc # didn't name it profit because that is our function's name! Don't want to clutter our name space!\n",
        "    return -1*pi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b1a329",
      "metadata": {
        "id": "56b1a329"
      },
      "source": [
        "### Making the call to `minimize`\n",
        "\n",
        "Now that our function is ready, it is time to minimize! The `minimize` function takes two arguments:\n",
        "1. Our function that we want to optimize\n",
        "2. A starting guess (as a vector)\n",
        "\n",
        "Let's give it a try."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d46c4fa",
      "metadata": {
        "id": "7d46c4fa"
      },
      "outputs": [],
      "source": [
        "res = minimize(profit, [0]) # provide function and starting inputs\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61532439",
      "metadata": {
        "id": "61532439"
      },
      "source": [
        "          fun: -28512.499999980355\n",
        "     hess_inv: array([[1.00000175]])\n",
        "          jac: array([0.])\n",
        "      message: 'Optimization terminated successfully.'\n",
        "         nfev: 21\n",
        "          nit: 3\n",
        "         njev: 7\n",
        "       status: 0\n",
        "      success: True\n",
        "            x: array([255.00019821])\n",
        "\n",
        "\n",
        "\n",
        "That's it! No calculus, no searching, no checking gradients manually. `minimize` simply takes our function and our starting guess and brings us back the optimal choice. We get lots of information stored in the attributes of the `res` object:\n",
        "\n",
        "- `fun` provides the value of the function (this is -1 times the profit level at the optimal output in our example)\n",
        "- `hess_inv` and `jac` are measures of gradient and are used to determine how far to go and in which direction\n",
        "- `message` should be self-explanatory\n",
        "- `nfev` is the number of times the function (in this case `profit`) was evaluated during the search\n",
        "- `nit` is the number of iterations it took to find the optimum\n",
        "- `njev` is the number of times the Jacobian was estimated\n",
        "- `status` is a code associated with the `message` and `success` atrributes\n",
        "- `success` tells you whether or not an optimum was found (sometimes it cannot be easily found!)\n",
        "- `x` probably the most important attribute. This tells us the optimal input value (in our case $Q$), or set of values depending on our function. In a regression context, this could represent the fitted coefficients!\n",
        "\n",
        "Going forward, you will realize (especially because so many of them print the output as they optimize) just how many libraries in Python use this minimize function behind the scenes. It is used in `statsmodels`, `sklearn`, and many other high-profile libraries! Now that you know where it really happens (in `scipy`!), you'll be better able to troubleshoot the problems that will inevitably arise as you use statistical models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4035bfcd",
      "metadata": {
        "id": "4035bfcd"
      },
      "source": [
        "**Solve-it!**\n",
        "\n",
        "In this lesson we learned about optimization using SciPy. For the assignment this week, I would like you to build off of your `RegressionModel` class. You will add a Logistic Regression (Logit) method to your class, so that when the `regression_type` parameter is `logit`, Logistic Regression Results are returned.\n",
        "\n",
        "Your job is to create the following functionality within your class object:\n",
        "- a method (call it `logistic_regression`) that estimates the results of logistic regression using your `x` and `y` data frames, and using a likelihood function and gradient descent (DO NOT USE PREBUILT REGRESSION FUNCTIONS).\n",
        "    - You need to write a function to calculate the Log-likelihood of your model\n",
        "    - You need to implement gradient descent to find the optimal values of beta\n",
        "    - You need to use your beta estimates, the model variance, and calculate the standard errors of the coefficients, as well as Z statistics and p-values\n",
        "    - the results should be stored in a dictionary named `results`, where each variable name (including the intercept if `create_intercept` is `True`) is the key, and the value is another dictionary, with keys for `coefficient`, `standard_error`, `z_stat`, and `p_value`. The coefficient should be the log odds-ratio (which takes the place of the coefficients in OLS)\n",
        "- a method called `fit_model` that uses the `self.regression_type` attribute to determine whether or not to run an OLS or Logistic Regression using the data provided. This method should call the correct regression method.\n",
        "- an updated method (call it `summary`) that presents your regression results in a table\n",
        "    - Columns should be: Variable name, Log odds-ratio value, standard error, z-statistic, and p-value, in that order.\n",
        "    - Your summary table should have different column titles for OLS and Logistic Regressions! (think if statement...)\n",
        "\n",
        "You only need to define the class. My code will create an instance of your class (be sure all the names match these instructions, and those from last week!), and provide data to run a regression. I will provide the same data to you, so that you can experiment and make sure that your code is functioning properly.\n",
        "\n",
        "NOTE: I have created a [primer on Logistic regression](https://github.com/dustywhite7/Econ8320/blob/master/SlidesPDF/9-2%20-%20Logit%20Primer.pdf) to go with this assignment. See the Github slidesPDF folder\n",
        "\n",
        "Put the code that you would like graded in the cell labeled `#si-exercise`. I recommend copying your code from the last assignment (in chapter 9 about linear regression and `numpy`), and continuing from there.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6a146533",
      "metadata": {
        "id": "6a146533"
      },
      "outputs": [],
      "source": [
        "#si-exercise\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import t\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "class RegressionModel(object):\n",
        "  def __init__(self, x, y, create_intercept = True, regression_type = ['ols', 'logit']):\n",
        "    if create_intercept:\n",
        "      x['intercept'] = 1\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.create_intercept = create_intercept\n",
        "    self.regression_type = regression_type\n",
        "    self.results = {}\n",
        "\n",
        "  def add_intercept(self):\n",
        "    if self.create_intercept == True:\n",
        "        if 'intercept' not in self.x.columns:\n",
        "            self.x['intercept'] = 1\n",
        "            self.x = self.x.reset_index(drop=True)\n",
        "        else:\n",
        "            print(\"Warning: 'intercept' variable already present.\")\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "  def fit_model(self):\n",
        "    if 'ols' in self.regression_type:\n",
        "        self.ols_regression()\n",
        "    if 'logit' in self.regression_type:\n",
        "        self.logistic_regression()\n",
        "\n",
        "  def ols_regression(self):\n",
        "    if self.create_intercept:\n",
        "      self.add_intercept()\n",
        "\n",
        "    beta = np.linalg.inv(self.x.T @ self.x) @ (self.x.T @ self.y)\n",
        "    df = len(self.y) - self.x.shape[1]\n",
        "    shat2 = ((self.y.T @ self.y) - ((self.y.T @ self.x) @ beta)) / df\n",
        "    cov_beta = shat2 * np.linalg.inv(self.x.T @ self.x)\n",
        "    variance = np.diag(cov_beta)\n",
        "    stan_err = np.sqrt(variance)\n",
        "    t_test = beta / stan_err\n",
        "    p_val = t.sf(t_test, df)\n",
        "\n",
        "    variable_names = list(self.x.columns)\n",
        "    if self.create_intercept:\n",
        "        if 'intercept' not in variable_names:\n",
        "            variable_names.append('intercept')\n",
        "\n",
        "    self.results = {}\n",
        "    for i, var_name in enumerate(variable_names):\n",
        "      self.results[var_name] = {\n",
        "          \"coefficient\": beta[i].tolist(),\n",
        "          \"standard_error\": stan_err[i].tolist(),\n",
        "          \"t_stat\": t_test[i].tolist(),\n",
        "          \"p_value\": p_val[i].tolist()\n",
        "            }\n",
        "\n",
        "  def logistic_regression(self):\n",
        "    if self.create_intercept:\n",
        "        self.add_intercept()\n",
        "\n",
        "    starting_beta = np.zeros((self.x.shape[0]))\n",
        "\n",
        "    def log_likelihood(beta):\n",
        "      beta = beta.reshape((-1, 1))  # Ensure beta is a column vector\n",
        "      linear_combination = np.sum(self.x * beta, axis=1)\n",
        "      p = 1 / (1 + np.exp(-linear_combination))\n",
        "      p = np.clip(p, 1e-10, 1 - 1e-10)  # Clip probabilities to prevent log(0) or log(1)\n",
        "      likelihood = np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
        "      return -likelihood\n",
        "\n",
        "\n",
        "\n",
        "    result = minimize(log_likelihood, starting_beta)\n",
        "\n",
        "    beta = result.x\n",
        "\n",
        "    p = (np.exp(self.x.T @ beta))/(1 + np.exp(self.x.T @ beta))\n",
        "\n",
        "    sigma2 = (len(self.y)*(p * (1 - p)))\n",
        "\n",
        "    serror = sigma2 * np.linalg.pinv(self.x.T @ self.x)\n",
        "\n",
        "    zstat = beta / np.sqrt(sigma2)\n",
        "\n",
        "    pval = stat.norm.sf(zstat)\n",
        "\n",
        "    self.results = dict()\n",
        "\n",
        "    for i in enumerate(self.x.columns):\n",
        "        self.results[i[1]] = {\n",
        "                'coefficient' : beta[i[0]],\n",
        "                'standard_error' : serror[i[0]],\n",
        "                't_stat' : zstat[i[0]],\n",
        "                'p_value' : pval[i[0]]\n",
        "                }\n",
        "\n",
        "\n",
        "  def summary(self):\n",
        "    if 'ols' in self.regression_type:\n",
        "      variable_stats = {}\n",
        "      for variable_name in self.x.columns:\n",
        "          stats = self.results[variable_name]\n",
        "          variable_stats[variable_name] = {\n",
        "              'coefficient value': stats[\"coefficient\"],\n",
        "              'standard error': stats[\"standard_error\"],\n",
        "              't-statistic': stats[\"t_stat\"],\n",
        "              'p-value': stats[\"p_value\"]\n",
        "          }\n",
        "      summary_df = pd.DataFrame(variable_stats).T\n",
        "      summary_df.index.name = 'Variable name'\n",
        "      summary_df.reset_index(inplace=True)\n",
        "      summary_df.columns = ['Variable name', 'coefficient value', 'standard error', 't-statistic', 'p-value']\n",
        "      print(summary_df)\n",
        "      return summary_df\n",
        "\n",
        "    elif 'logit' in self.regression_type:\n",
        "      variable_stats = {}\n",
        "      for variable_name in self.x.columns:\n",
        "          stats = self.results[variable_name]\n",
        "          variable_stats[variable_name] = {\n",
        "              'Log odds-ratio value': stats[\"coefficient\"],\n",
        "              'standard error': stats[\"standard_error\"],\n",
        "              'z-statistic': stats[\"z_stat\"],\n",
        "              'p-value': stats[\"p_value\"]\n",
        "          }\n",
        "\n",
        "      summary_df = pd.DataFrame(variable_stats).T\n",
        "      summary_df.index.name = 'Variable name'\n",
        "      summary_df.reset_index(inplace=True)\n",
        "      summary_df.columns = ['Variable name', 'coefficient value', 'standard error', 'z-statistic', 'p-value']\n",
        "      print(summary_df)\n",
        "      return summary_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/ECON 8320/Week 11 - Optimization/assignment8Data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "x = data[['sex','age','educ']]\n",
        "y = data['white']\n",
        "reg = RegressionModel(x, y, create_intercept=True, regression_type = ['logit'])\n",
        "reg.fit_model()\n",
        "reg.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "A7UHeW1Axvna",
        "outputId": "4d5b835c-e1d9-48ff-9b6e-c1401a3618cc"
      },
      "id": "A7UHeW1Axvna",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-b2b00f54cc42>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x['intercept'] = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 'intercept' variable already present.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-981c54a0b33a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegressionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'logit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-b2b00f54cc42>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mols_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'logit'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mols_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-b2b00f54cc42>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_beta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0mmaxiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m     sf = _prepare_scalar_function(fun, x0, jac, args=args, epsilon=eps,\n\u001b[0m\u001b[1;32m   1420\u001b[0m                                   finite_diff_rel_step=finite_diff_rel_step)\n\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;31m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0m\u001b[1;32m    384\u001b[0m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# Hessian Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n\u001b[0m\u001b[1;32m    174\u001b[0m                                            **finite_diff_options)\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparsity\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             return _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[0m\u001b[1;32m    506\u001b[0m                                      use_one_sided, method)\n\u001b[1;32m    507\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Recompute dx as exactly representable number.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'3-point'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_one_sided\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             raise RuntimeError(\"`fun` return value has \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;31m# Make sure the function returns a true scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-b2b00f54cc42>\u001b[0m in \u001b[0;36mlog_likelihood\u001b[0;34m(beta)\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0mlinear_combination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlinear_combination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m       \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clip probabilities to prevent log(0) or log(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m       \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m     \"\"\"\n\u001b[0;32m-> 2169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(self, lower, upper, axis, inplace, **kwargs)\u001b[0m\n\u001b[1;32m   5926\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5927\u001b[0m     ) -> Series | None:\n\u001b[0;32m-> 5928\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5930\u001b[0m     def interpolate(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(self, lower, upper, axis, inplace, **kwargs)\u001b[0m\n\u001b[1;32m   8213\u001b[0m             \u001b[0mupper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8214\u001b[0m         ):\n\u001b[0;32m-> 8215\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clip_with_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8217\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_clip_with_scalar\u001b[0;34m(self, lower, upper, inplace)\u001b[0m\n\u001b[1;32m   8023\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8024\u001b[0m                 \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8025\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8026\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8027\u001b[0m                 \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(self, cond, other, inplace, axis, level)\u001b[0m\n\u001b[1;32m   5996\u001b[0m         \u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5997\u001b[0m     ) -> Series | None:\n\u001b[0;32m-> 5998\u001b[0;31m         return super().where(\n\u001b[0m\u001b[1;32m   5999\u001b[0m             \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6000\u001b[0m             \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(self, cond, other, inplace, axis, level)\u001b[0m\n\u001b[1;32m   9931\u001b[0m         \"\"\"\n\u001b[1;32m   9932\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9933\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_where\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9935\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_where\u001b[0;34m(self, cond, other, inplace, axis, level)\u001b[0m\n\u001b[1;32m   9733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9734\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9735\u001b[0;31m             new_data = self._mgr.where(\n\u001b[0m\u001b[1;32m   9736\u001b[0m                 \u001b[0mother\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9737\u001b[0m                 \u001b[0mcond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(self, other, cond, align)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    366\u001b[0m             \u001b[0;34m\"where\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0malign_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(self, other, cond, _downcast, using_cow)\u001b[0m\n\u001b[1;32m   1171\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0micond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_putmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;31m# GH-39595: Always return a copy; short-circuit up/downcasting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/array_algos/putmask.py\u001b[0m in \u001b[0;36mvalidate_putmask\u001b[0;34m(values, mask)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m def validate_putmask(\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArrayLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m ) -> tuple[npt.NDArray[np.bool_], bool]:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sex = {'coefficient': -1.1229156890097627,\n",
        "      'standard_error': 0.39798772782618025,\n",
        "      'z_stat': -2.821483202869492,\n",
        "      'p_value': 0.004780214077269219}\n",
        "age = {'coefficient': -0.007012518056833769,\n",
        "        'standard_error': 0.010835821823286998,\n",
        "        'z_stat': -0.6471607019011091,\n",
        "        'p_value': 0.5175279421902776}\n",
        "educ = {'coefficient': -0.046485475816343394,\n",
        "        'standard_error': 0.10100278092776117,\n",
        "        'z_stat': -0.46023956359766527,\n",
        "        'p_value': 0.6453442758780246}\n",
        "intercept = {'coefficient': 5.735435005488546,\n",
        "            'standard_error': 1.1266207023561843,\n",
        "            'z_stat': 5.090830475148922,\n",
        "            'p_value': 3.56498650369634e-07}"
      ],
      "metadata": {
        "id": "rKSBeAe9BDhr"
      },
      "id": "rKSBeAe9BDhr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys as system\n",
        "import io\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from scipy.stats import t\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/ECON 8320/Week 11 - Optimization/assignment8Data.csv'\n",
        "data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StHg5hUsxbWR",
        "outputId": "fdc0603e-3917-432a-d363-950caaac92e8"
      },
      "id": "StHg5hUsxbWR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tests\n",
        "class TestCase(unittest.TestCase):\n",
        "  def testLogitCorrectValues(self):\n",
        "        data = pd.read_csv(file_path)\n",
        "        x = data.loc[:100, ['sex','age','educ']]\n",
        "        y = data.loc[:100, 'white']\n",
        "        reg = RegressionModel(x, y, create_intercept=True, regression_type='logit')\n",
        "        reg.fit_model()\n",
        "\n",
        "        sex = {'coefficient': -1.1229156890097627,\n",
        "        'standard_error': 0.39798772782618025,\n",
        "        'z_stat': -2.821483202869492,\n",
        "        'p_value': 0.004780214077269219}\n",
        "        age = {'coefficient': -0.007012518056833769,\n",
        "        'standard_error': 0.010835821823286998,\n",
        "        'z_stat': -0.6471607019011091,\n",
        "        'p_value': 0.5175279421902776}\n",
        "        educ = {'coefficient': -0.046485475816343394,\n",
        "        'standard_error': 0.10100278092776117,\n",
        "        'z_stat': -0.46023956359766527,\n",
        "        'p_value': 0.6453442758780246}\n",
        "        intercept = {'coefficient': 5.735435005488546,\n",
        "        'standard_error': 1.1266207023561843,\n",
        "        'z_stat': 5.090830475148922,\n",
        "        'p_value': 3.56498650369634e-07}\n",
        "        sexEq = (round(sex['coefficient'], 1)==round(reg.results['sex']['coefficient'], 1)) & (round(sex['standard_error'], 1)==round(reg.results['sex']['standard_error'], 1)) & (round(sex['z_stat'], 1)==round(reg.results['sex']['z_stat'], 1)) & (round(sex['p_value'], 1)==round(reg.results['sex']['p_value'], 1))\n",
        "        ageEq = (round(age['coefficient'], 1)==round(reg.results['age']['coefficient'], 1)) &  (round(age['standard_error'], 1)==round(reg.results['age']['standard_error'], 1)) & (round(age['z_stat'], 1)==round(reg.results['age']['z_stat'], 1)) & (round(age['p_value'], 1)==round(reg.results['age']['p_value'], 1))\n",
        "        educEq = (round(educ['coefficient'], 1)==round(reg.results['educ']['coefficient'], 1)) & (round(educ['standard_error'], 1)==round(reg.results['educ']['standard_error'], 1)) & (round(educ['z_stat'], 1)==round(reg.results['educ']['z_stat'], 1)) & (round(educ['p_value'], 1)==round(reg.results['educ']['p_value'], 1))\n",
        "        interceptEq = (round(intercept['coefficient'], 1)==round(reg.results['intercept']['coefficient'], 1)) & (round(intercept['standard_error'], 1)==round(reg.results['intercept']['standard_error'], 1)) & (round(intercept['z_stat'], 1)==round(reg.results['intercept']['z_stat'], 1)) & (round(intercept['p_value'], 1)==round(reg.results['intercept']['p_value'], 1))\n",
        "\n",
        "        sexEq2 = (round(sex['coefficient'], 1)==round(reg.results['sex']['coefficient'], 1)) & (round(sex['standard_error'], 1)==round(reg.results['sex']['standard_error'], 1)) & (round(sex['z_stat'], 1)==round(reg.results['sex']['z_stat'], 1)) & (round(sex['p_value']/2, 1)==round(reg.results['sex']['p_value'], 1))\n",
        "        ageEq2 = (round(age['coefficient'], 1)==round(reg.results['age']['coefficient'], 1)) &  (round(age['standard_error'], 1)==round(reg.results['age']['standard_error'], 1)) & (round(age['z_stat'], 1)==round(reg.results['age']['z_stat'], 1)) & (round(age['p_value']/2, 1)==round(reg.results['age']['p_value'], 1))\n",
        "        educEq2 = (round(educ['coefficient'], 1)==round(reg.results['educ']['coefficient'], 1)) & (round(educ['standard_error'], 1)==round(reg.results['educ']['standard_error'], 1)) & (round(educ['z_stat'], 1)==round(reg.results['educ']['z_stat'], 1)) & (round(educ['p_value']/2, 1)==round(reg.results['educ']['p_value'], 1))\n",
        "        interceptEq2 = (round(intercept['coefficient'], 1)==round(reg.results['intercept']['coefficient'], 1)) & (round(intercept['standard_error'], 1)==round(reg.results['intercept']['standard_error'], 1)) & (round(intercept['z_stat'], 1)==round(reg.results['intercept']['z_stat'], 1)) & (round(intercept['p_value']/2, 1)==round(reg.results['intercept']['p_value'], 1))\n",
        "\n",
        "        self.assertTrue((sexEq & ageEq & educEq & interceptEq)|(sexEq2 & ageEq2 & educEq2 & interceptEq2), \"Your coefficients are not corretly calculated.\")"
      ],
      "metadata": {
        "id": "GfAcwm8swSWY"
      },
      "id": "GfAcwm8swSWY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestCase(unittest.TestCase):\n",
        "\n",
        "    def testSummaryTableOutput(self):\n",
        "      stdout = system.stdout\n",
        "      system.stdout = io.StringIO()\n",
        "\n",
        "\n",
        "      data = pd.read_csv(file_path)\n",
        "      x = data.loc[:100, ['sex','age','educ']]\n",
        "      y = data.loc[:100, 'white']\n",
        "      reg = RegressionModel(x, y, create_intercept=True, regression_type='logit')\n",
        "      reg.fit_model()\n",
        "      reg.summary()\n",
        "\n",
        "      output = system.stdout.getvalue()\n",
        "      system.stdout = stdout\n",
        "\n",
        "      labels = bool(re.findall(r'[Cc]oef.*[Ss]t.*[Zz].*[Pp].*', output))\n",
        "      sex = bool(re.findall(r'sex.*\\d+.*\\d+.*\\d+.*\\d+.*', output))\n",
        "      age = bool(re.findall(r'age.*\\d+.*\\d+.*\\d+.*\\d+.*', output))\n",
        "      educ = bool(re.findall(r'educ.*\\d+.*\\d+.*\\d+.*\\d+.*', output))\n",
        "      intercept = bool(re.findall(r'intercept.*\\d+.*\\d+.*\\d+.*\\d+.*', output))\n",
        "\n",
        "      self.assertTrue(labels&sex&age&educ&intercept, \"Your table is not correctly formatted.\")"
      ],
      "metadata": {
        "id": "aU589OHvXk7_"
      },
      "id": "aU589OHvXk7_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your job is to create the following functionality within your class object:\n",
        "- a method (call it `logistic_regression`) that estimates the results of logistic regression using your `x` and `y` data frames, and using a likelihood function and gradient descent (DO NOT USE PREBUILT REGRESSION FUNCTIONS).\n",
        "    - You need to write a function to calculate the Log-likelihood of your model\n",
        "    - You need to implement gradient descent to find the optimal values of beta\n",
        "    - You need to use your beta estimates, the model variance, and calculate the standard errors of the coefficients, as well as Z statistics and p-values\n",
        "    - the results should be stored in a dictionary named `results`, where each variable name (including the intercept if `create_intercept` is `True`) is the key, and the value is another dictionary, with keys for `coefficient`, `standard_error`, `z_stat`, and `p_value`. The coefficient should be the log odds-ratio (which takes the place of the coefficients in OLS)\n",
        "- a method called `fit_model` that uses the `self.regression_type` attribute to determine whether or not to run an OLS or Logistic Regression using the data provided. This method should call the correct regression method.\n",
        "- an updated method (call it `summary`) that presents your regression results in a table\n",
        "    - Columns should be: Variable name, Log odds-ratio value, standard error, z-statistic, and p-value, in that order.\n",
        "    - Your summary table should have different column titles for OLS and Logistic Regressions! (think if statement...)"
      ],
      "metadata": {
        "id": "r-j9Qz2SgNWQ"
      },
      "id": "r-j9Qz2SgNWQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stat\n",
        "import pandas as pd\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "class RegressionModel(object):\n",
        "  def __init__(self, x, y, create_intercept=True, regression_type= ['ols', 'logit']):\n",
        "      self.results = {}\n",
        "      if isinstance(x, pd.DataFrame):\n",
        "          self.x = x\n",
        "      else:\n",
        "          raise RuntimeError(\"Matrix 'x' is not a DataFrame.\")\n",
        "      if isinstance(y, pd.DataFrame) | isinstance(y, pd.Series):\n",
        "          self.y = y\n",
        "      else:\n",
        "          raise RuntimeError(\"Matrix 'y' is not a DataFrame.\")\n",
        "      if isinstance(create_intercept, bool):\n",
        "          self.create_intercept = create_intercept\n",
        "          if self.create_intercept:\n",
        "              self.add_intercept()\n",
        "      else:\n",
        "          raise RuntimeError(\"Parameter 'create_intercept' must be a boolean value.\")\n",
        "      if isinstance(regression_type, str):\n",
        "          if regression_type==\"ols\":\n",
        "              self.regression_type=regression_type\n",
        "          elif regression_type=='logit':\n",
        "              self.regression_type=regression_type\n",
        "          else:\n",
        "              raise RuntimeError(\"Only OLS and Logistic regressions ('ols' or 'logit', respectively) are supported\")\n",
        "      else:\n",
        "          raise RuntimeError(\"Parameter 'regression_type' must be a string with value 'ols' or 'logit'.\")\n",
        "\n",
        "  def fit_model(self):\n",
        "      if 'ols' in self.regression_type:\n",
        "          self.ols_regression()\n",
        "      elif 'logit' in self.regression_type:\n",
        "          self.logistic_regression()\n",
        "          print(\"Success in logit choice\")\n",
        "      else:\n",
        "          print(\"No valid regression type chosen\")\n",
        "\n",
        "  def add_intercept(self):\n",
        "      self.x = self.x.assign(intercept=pd.Series([1]*np.shape(self.x)[0]))\n",
        "\n",
        "  def ols_regression(self):\n",
        "    x = self.x\n",
        "    y = self.y\n",
        "    n, k = np.shape(x)\n",
        "    beta = np.dot(np.linalg.solve(x.T.dot(x), np.eye(k)), x.T.dot(y))\n",
        "    eps = y - x.dot(beta)\n",
        "    shat = eps.T.dot(eps)/(n-k)\n",
        "    covar = shat * np.linalg.solve(x.T.dot(x), np.eye(k))\n",
        "    var = np.diag(covar)\n",
        "    serror = np.asarray([np.sqrt(i) for i in var])\n",
        "    tstat = np.asarray([i[1]/serror[i[0]] for i in enumerate(beta)])\n",
        "    pval = stat.t.sf(tstat, n-k)\n",
        "\n",
        "    self.results = dict()\n",
        "\n",
        "    for i in enumerate(self.x.columns):\n",
        "        self.results[i[1]] = {\n",
        "                'coefficient' : beta[i[0]],\n",
        "                'standard_error' : serror[i[0]],\n",
        "                't_stat' : tstat[i[0]],\n",
        "                'p_value' : pval[i[0]]\n",
        "                }\n",
        "  def logistic_regression(self):\n",
        "    if self.create_intercept:\n",
        "        self.add_intercept()\n",
        "\n",
        "    starting_beta = np.zeros((self.x.shape[0]))\n",
        "\n",
        "    def log_likelihood(beta):\n",
        "      beta = beta.reshape((-1, 1))  # Ensure beta is a column vector\n",
        "      linear_combination = np.sum(self.x * beta, axis=1)\n",
        "      p = 1 / (1 + np.exp(-linear_combination))\n",
        "      p = np.clip(p, 1e-10, 1 - 1e-10)  # Clip probabilities to prevent log(0) or log(1)\n",
        "      likelihood = np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
        "      return -likelihood\n",
        "\n",
        "\n",
        "\n",
        "    result = minimize(log_likelihood, starting_beta)\n",
        "\n",
        "    beta = result.x\n",
        "\n",
        "    p = (np.exp(self.x.T @ beta))/(1 + np.exp(self.x.T @ beta))\n",
        "\n",
        "    sigma2 = (len(self.y)*(p * (1 - p)))\n",
        "\n",
        "    serror = sigma2 * np.linalg.pinv(self.x.T @ self.x)\n",
        "\n",
        "    zstat = beta / np.sqrt(sigma2)\n",
        "\n",
        "    pval = stat.norm.sf(zstat)\n",
        "\n",
        "    self.results = dict()\n",
        "\n",
        "    for i in enumerate(self.x.columns):\n",
        "        self.results[i[1]] = {\n",
        "                'coefficient' : beta[i[0]],\n",
        "                'standard_error' : serror[i[0]],\n",
        "                't_stat' : zstat[i[0]],\n",
        "                'p_value' : pval[i[0]]\n",
        "                }\n",
        "\n",
        "  def summary(self):\n",
        "    if 'ols' in self.regression_type:\n",
        "      print(\"Variable Name\".ljust(25) + \"| Coefficient\".ljust(15) + \"| Standard Error\".ljust(17) + \"| T-Statistic\".ljust(15) + \"| P-Value\".ljust(15) + \"\\n\" + \"-\"*85)\n",
        "      for i in self.results:\n",
        "          print(\"{}\".format(i).ljust(25) + \"| {}\".format(round(self.results[i]['coefficient'], 3)).ljust(15) + \"| {}\".format(round(self.results[i]['standard_error'], 3)).ljust(17) + \"| {}\".format(round(self.results[i]['t_stat'], 3)).ljust(15) + \"| {}\".format(round(self.results[i]['p_value'], 3)).ljust(15))\n",
        "\n",
        "    if 'logit' in self.regression_type:\n",
        "      print(\"Variable Name\".ljust(25) + \"| Coefficient\".ljust(15) + \"| Standard Error\".ljust(17) + \"| Z-Statistic\".ljust(15) + \"| P-Value\".ljust(15) + \"\\n\" + \"-\"*85)\n",
        "      for i in self.results:\n",
        "          print(\"{}\".format(i).ljust(25) + \"| {}\".format(round(self.results[i]['coefficient'], 3)).ljust(15) + \"| {}\".format(round(self.results[i]['standard_error'], 3)).ljust(17) + \"| {}\".format(round(self.results[i]['z_stat'], 3)).ljust(15) + \"| {}\".format(round(self.results[i]['p_value'], 3)).ljust(15))"
      ],
      "metadata": {
        "id": "r0_18q-jWwwd"
      },
      "id": "r0_18q-jWwwd",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/ECON 8320/Week 11 - Optimization/assignment8Data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "x = data[['sex','age','educ']]\n",
        "y = data['white']\n",
        "reg = RegressionModel(x, y, create_intercept=True, regression_type = 'logit')\n",
        "reg.fit_model()\n",
        "reg.summary()"
      ],
      "metadata": {
        "id": "XvwBAASMZDkl"
      },
      "id": "XvwBAASMZDkl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sex = {'coefficient': -1.1229156890097627,\n",
        "      'standard_error': 0.39798772782618025,\n",
        "      'z_stat': -2.821483202869492,\n",
        "      'p_value': 0.004780214077269219}\n",
        "age = {'coefficient': -0.007012518056833769,\n",
        "        'standard_error': 0.010835821823286998,\n",
        "        'z_stat': -0.6471607019011091,\n",
        "        'p_value': 0.5175279421902776}\n",
        "educ = {'coefficient': -0.046485475816343394,\n",
        "        'standard_error': 0.10100278092776117,\n",
        "        'z_stat': -0.46023956359766527,\n",
        "        'p_value': 0.6453442758780246}\n",
        "intercept = {'coefficient': 5.735435005488546,\n",
        "            'standard_error': 1.1266207023561843,\n",
        "            'z_stat': 5.090830475148922,\n",
        "            'p_value': 3.56498650369634e-07}"
      ],
      "metadata": {
        "id": "xkrtWpUrZEEs"
      },
      "id": "xkrtWpUrZEEs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}